# -*- coding: utf-8 -*-
"""Sigmoid_Healthcare_Dataset

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_inloXO91iZwGJHIJSBpvbFukVX-39VR

# **ASSUMPTIONS**


A first i wll atentevely analyse the factors and build my own dependencies which I consider correct and have the most poverful correlation and influence between them.

In my opinion the most strong correlation would be between pregnancies and level of glucose because I know that there is a correlation between pregnancy and risk of diabetes. I guess that the analysis would prove my opinion.

I will make data analysis, vizualisation for easier understanding the data and predictions.
"""

import kagglehub
import pandas as pd
import numpy as np
import os
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

path = kagglehub.dataset_download("nanditapore/healthcare-diabetes")

files = os.listdir(path)
csv_files = [file for file in files if file.endswith('.csv')]
data_path = os.path.join(path, csv_files[0])  # Assuming there's one main dataset

df = pd.read_csv(data_path)

print("Dataset Overview:")
print(df.head())
print("\nMissing Values:\n", df.isnull().sum())

"""# Data Cleaning and Preprocessing

I will drop the column Id for data analysis because it should not make any correlation between the factors.

The other valuable steps are cleaning and prepariong the data for use, graphs and clear visualisations.
"""

df.dropna(inplace=True)
df=df.drop(columns=["Id"])


categorical_columns = df.select_dtypes(include=['object']).columns
for col in categorical_columns:
    df[col] = LabelEncoder().fit_transform(df[col])

# Correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, cmap="coolwarm")
plt.title("Feature Correlation Matrix")
plt.show()

"""Correlation Matrix is a standart way to show dependences between the factors and there influence one over another factor"""

df.hist(figsize=(15, 8))

"""Pairplot to visualize dependencies can be used for more easier understatnding of correlations and influence between facctors."""

sns.pairplot(df, hue="Outcome", diag_kind='hist', palette="husl")
plt.show()

"""A boxplot provides a summary of a numerical featureâ€™s distribution, including outliers. It helps in understanding the spread of data and finding potential extreme values that may need special handling"""

plt.figure(figsize=(14, 8))
sns.boxplot(data=df, palette="coolwarm")
plt.xticks(rotation=45)
plt.title("Boxplot of Features (Outliers Detection)")
plt.show()

# Splitting data into X and Y
X = df.drop(columns=['Outcome'])  #Outcome there is or not
y = df['Outcome']

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Model Training
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Model Evaluation
accuracy = accuracy_score(y_test, y_pred)
print("\nModel Accuracy:", accuracy)

"""We can see a pretty good exaple on model accracy as the result."""

print("\nClassification Report:\n", classification_report(y_test, y_pred))

"""Classification_report helps understand how well the model distinguishes between different classes"""

print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

"""The confusion matrix helps us evaluate model mistakes and decide what to improve:


- If FP is high: Too many false alarms => Model needs better precision (avoid diagnosing healthy people as diabetic).

- If FN is high: Dangerous! The model is missing real diabetic cases => We need better recall.
"""

# Feature Importance
feature_importance = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)
plt.figure(figsize=(10, 5))
sns.barplot(x=feature_importance, y=feature_importance.index)
plt.title("Feature Importance")
plt.show()

"""# **CONCLUSION**

In the end I can figure out few dependencies:


If Glucose levels are strongly correlated with the outcome, it suggests that higher glucose levels are a strong predictor of diabetes.

If Body Mass Index and Insulin show some correlation, it may indicate that higher Body Mass Index tends to be associated with insulin resistance.

If Age is weakly correlated, it might mean that diabetes can occur at any age, though risk may increase with age.

I can tell that my assumptions were not accurate. The corellations between factors are more accuratly described and visualized in tabels.

That analysis can be aplied for medical use. Especcially in predicting risks.
"""